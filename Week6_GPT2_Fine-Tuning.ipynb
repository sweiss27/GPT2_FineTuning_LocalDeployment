{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# GPT-2 Lyrics Generation: LoRA Fine-Tuning, ONNX Export, and Local Gradio Deployment\n\nThis notebook fine-tunes **GPT-2** on a song lyrics dataset, exports a lightweight **ONNX** model, deploys a local **Gradio** app, and evaluates output quality using **perplexity**, **BLEU**, and qualitative review.\n\n**Key outputs (saved to `./artifacts/<run_id>/`):** run metrics, training logs, tuning sweep results, ONNX export info, and BLEU samples.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Environment setup\nInstall libraries and set environment variables.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "04a8acad-a7ae-45f8-99fe-032129d9927c",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (4.5.0)\n",
            "Requirement already satisfied: transformers in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (4.57.6)\n",
            "Requirement already satisfied: peft in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (0.18.1)\n",
            "Requirement already satisfied: accelerate in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (1.12.0)\n",
            "Requirement already satisfied: evaluate in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (0.4.6)\n",
            "Requirement already satisfied: nltk in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: onnx in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (1.20.1)\n",
            "Requirement already satisfied: onnxruntime in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (1.24.1)\n",
            "Requirement already satisfied: gradio in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (6.5.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: optimum[onnxruntime] in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (2.1.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (2.3.5)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (23.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (2.32.5)\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (0.36.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: anyio in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from peft) (7.0.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from peft) (2.10.0)\n",
            "Requirement already satisfied: click in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from onnx) (6.33.5)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from onnx) (0.5.4)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from onnxruntime) (25.12.19)\n",
            "Requirement already satisfied: sympy in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from onnxruntime) (1.14.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.129.0)\n",
            "Requirement already satisfied: ffmpy in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==2.0.3 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: groovy~=0.1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: jinja2<4.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (3.11.7)\n",
            "Requirement already satisfied: pillow<13.0,>=8.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<=3.0,>=2.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (2.12.5)\n",
            "Requirement already satisfied: pydub in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.0.22)\n",
            "Requirement already satisfied: pytz>=2017.2 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (2025.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.52.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.23.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.4.2)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.3.2)\n",
            "Requirement already satisfied: optimum-onnx[onnxruntime] in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from optimum[onnxruntime]) (0.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from torch>=1.13.0->peft) (3.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shweiss\\appdata\\local\\anaconda3\\envs\\textml\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers peft accelerate evaluate nltk onnx onnxruntime gradio optimum[onnxruntime] pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "31584476-0dc6-484f-be40-f081ea53a47d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Imports and reproducibility\nImports, random seeds, and artifact/output folders.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "550b8973-969a-4415-b6af-2b9d266ea257",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback,\n",
        "    pipeline,\n",
        ")\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be49d80",
      "metadata": {},
      "source": "### Reproducibility and artifact folders\nThis section sets seeds and creates an `artifacts/<run_id>/` folder to store logs, metrics, and evaluation outputs.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "584f1dad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN_DIR: C:\\Users\\shweiss\\Downloads\\artifacts\\20260221_113232\n"
          ]
        }
      ],
      "source": [
        "import json, math, random, time\n",
        "from datetime import datetime\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "try:\n",
        "    import numpy as np\n",
        "    np.random.seed(SEED)\n",
        "except Exception:\n",
        "    pass\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Artifact folders\n",
        "ARTIFACT_ROOT = Path(\"./artifacts\")\n",
        "ARTIFACT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RUN_DIR = ARTIFACT_ROOT / RUN_ID\n",
        "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ADAPTER_DIR = Path(\"./gpt2-lyrics-lora-adapter\")\n",
        "MERGED_DIR  = Path(\"./gpt2-lyrics-merged\")\n",
        "ONNX_DIR    = Path(\"./gpt2-lyrics-onnx\")\n",
        "\n",
        "print(\"RUN_DIR:\", RUN_DIR.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Dataset selection and preprocessing\nLoad the lyrics dataset, select the text field, clean rows, and tokenize for GPT-2.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f58da096-d148-4126-a932-0bc222a59586",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lyrics', 'genre']\n",
            "{'lyrics': \"[Intro: Method Man w/ sample] + (Sunny valentine). We got butter (8X). (The gun'll go the gun'll go.... The gun'll go...). [Raekwon]. Aiyo one thing for sure keep you of all. Keep a nice crib fly away keep to the point. Keep niggaz outta ya face who snakes. Keep bitches in they place keep the mac in a special place. Keep moving for papes keep cool keep doing what you doing. Keep it fly keep me in the crates. Cuz I will erase shit on the real note you'se a waste. It's right here for you I will lace you. Rip you and brace you put a nice W up on ya face. Word to mother you could get chased. It's nothing to taste blood on a thug if he gotta go. All I know is we be giving grace. This is a place from where we make tapes. We make 'em everywhere still in all we be making base. Y'all be making paste these little niggaz they be making shapes. Our shit is art yours is traced. [Chorus: Sunny Valentine]. This is the way that we rolling in the streets. You know when we roll we be packing that heat. The gun'll go the gun'll go the gun'll go the gun'll go. The gun'll go the gun'll go the gun'll go the gun'll go. The gun'll go the gun'll go.... [Method Man]. This is Poverty Island man these animals don't run. Slums where the ambulance don't come. Who got the best base? Fiends waiting to smoke some. Approach something ask him where he getting that coke from. My dudes hug blocks like samurai shogun. Cuz no V and no ones equalling no fun. Who want a treat they know huh? Body to go numb. My woman need funds plus her hair and her toes done. It is what it is though you fuck with the kid flow. That make it hard to get dough the harder to get gold. Harder the piff blow harder when it snow. The pinky and the wrist glow this here what we live for. Get gwop then get low but first thought. We gotta get the work off the gift and the curse boss. Yeah see I'm the shit yo the dirt in the fit no. Hustling from the get-go the motto is get more. [Chorus]. [Masta Killa]. We was quiet flashy brothers strapped all along. With the dirty .38 long twelve hour shift gate. Took case state to state you think he won't hold his weight?. Put ya money on the plate and watch it get scrapped. We get ape up in that club off that juice and Henn. And it's a no win situation fucking with them. You mean like Ewing at the front at the rim finger roll a Dutch. Million dollar stages touched techs gauges bust. Trust no one the lone shogun rugged Timb boot stomper. Damaging lyrical mass destruction launcher. Nothing can calm the quakeage when I break kid. Peace to my brothers up north doing state bids. [Chorus]. [Chorus 2: Sunny Valentine]. Whoa... this is the way we be rolling in the club. You know when we roll we be packing .32 snubs. The gun'll go the gun'll go the gun'll go the gun'll go. The gun'll go the gun'll go the gun'll go the gun'll go. The gun'll go the gun'll go the gun'll go the gun'll go. [Outro: sample to fade]. We got butter...\", 'genre': 'Hip Hop'}\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"halaction/song-lyrics\", split=\"train[:1000]\")\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "afb371e8-8b72-4f17-8097-8ef5356db894",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8817d3a3c05041cfb352e1788e1a4c2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using text column: lyrics\n",
            "Rows: 1000\n"
          ]
        }
      ],
      "source": [
        "candidate_columns = [\"lyrics\", \"text\", \"song\", \"content\"]\n",
        "text_col = next((c for c in candidate_columns if c in dataset.column_names), dataset.column_names[0])\n",
        "\n",
        "dataset = dataset.filter(lambda x: x[text_col] is not None and x[text_col].strip() != \"\")\n",
        "dataset = dataset.select_columns([text_col])\n",
        "print(\"Using text column:\", text_col)\n",
        "print(\"Rows:\", len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2d9fa5-59b8-4c6c-975a-552189889e0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 pad token fix\n",
        "max_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bb40c406-c150-4f4b-8d56-828a977f8b57",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b256a781632545b9a730d3b61877e9b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def tokenize_function(batch):\n",
        "    # Keep a copy of the original text for later evaluation (BLEU / qualitative checks)\n",
        "    raw_texts = batch[text_col]\n",
        "    texts = [t + tokenizer.eos_token for t in raw_texts]\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Causal LM labels\n",
        "    tokenized[\"raw_text\"] = raw_texts\n",
        "    return tokenized\n",
        "\n",
        "tokenized_data = dataset.map(tokenize_function, batched=True, remove_columns=[text_col])\n",
        "tokenized_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "splits = tokenized_data.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = splits[\"train\"]\n",
        "eval_dataset = splits[\"test\"]\n",
        "\n",
        "print(train_dataset[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. LoRA fine-tuning and training optimization\nApply LoRA, configure training with weight decay and early stopping, and train/evaluate.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4f44ee07-1314-48e7-baf6-7a3e5ded87b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2518ad35",
      "metadata": {},
      "source": "### Hyperparameter tuning evidence\nTo satisfy the tuning requirement, I ran a small learning-rate sweep (2 quick trials) and recorded eval loss and perplexity for comparison.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a451d428",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shweiss\\AppData\\Local\\Temp\\ipykernel_37192\\2877250962.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  t = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 04:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.821200</td>\n",
              "      <td>3.358577</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 03:59, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.835800</td>\n",
              "      <td>3.372452</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:27]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>eval_loss</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>seconds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lr_2e-4</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>3.358577</td>\n",
              "      <td>28.748253</td>\n",
              "      <td>278.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lr_1e-4</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>3.372452</td>\n",
              "      <td>29.149902</td>\n",
              "      <td>286.10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      name  learning_rate  eval_loss  perplexity  seconds\n",
              "0  lr_2e-4         0.0002   3.358577   28.748253   278.73\n",
              "1  lr_1e-4         0.0001   3.372452   29.149902   286.10"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEST_LR selected for final run: 0.0002\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "RUN_SWEEP = True      # set False to skip\n",
        "SWEEP_ROWS = 200      # keep small for speed\n",
        "SWEEP_EPOCHS = 1\n",
        "\n",
        "# Two simple variants (learning rate). You can add more if you want.\n",
        "SWEEP_CONFIGS = [\n",
        "    {\"name\": \"lr_2e-4\", \"learning_rate\": 2e-4},\n",
        "    {\"name\": \"lr_1e-4\", \"learning_rate\": 1e-4},\n",
        "]\n",
        "\n",
        "sweep_results = []\n",
        "\n",
        "def run_quick_trial(cfg):\n",
        "    # fresh base model each trial\n",
        "    base = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    base.resize_token_embeddings(len(tokenizer))\n",
        "    base.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # same LoRA config (you can also sweep r/alpha if you want)\n",
        "    trial_lora = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=lora_config.r,\n",
        "        lora_alpha=lora_config.lora_alpha,\n",
        "        lora_dropout=lora_config.lora_dropout,\n",
        "        target_modules=lora_config.target_modules,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "    trial_model = get_peft_model(base, trial_lora)\n",
        "\n",
        "    # tiny subset for speed\n",
        "    tiny_train = train_dataset.select(range(min(SWEEP_ROWS, len(train_dataset))))\n",
        "    tiny_eval  = eval_dataset.select(range(min(int(SWEEP_ROWS*0.25), len(eval_dataset))))\n",
        "\n",
        "    trial_args = dict(\n",
        "        output_dir=str(RUN_DIR / f\"sweep_{cfg['name']}\"),\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=cfg[\"learning_rate\"],\n",
        "        num_train_epochs=SWEEP_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        ta = TrainingArguments(evaluation_strategy=\"epoch\", **trial_args)\n",
        "    except TypeError:\n",
        "        ta = TrainingArguments(eval_strategy=\"epoch\", **trial_args)\n",
        "\n",
        "    t = Trainer(\n",
        "        model=trial_model,\n",
        "        args=ta,\n",
        "        train_dataset=tiny_train,\n",
        "        eval_dataset=tiny_eval,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    t.train()\n",
        "    m = t.evaluate()\n",
        "    secs = time.time() - start\n",
        "\n",
        "    eval_loss = float(m.get(\"eval_loss\", float(\"nan\")))\n",
        "    ppl = float(math.exp(eval_loss)) if eval_loss == eval_loss else float(\"nan\")\n",
        "\n",
        "    return {\n",
        "        \"name\": cfg[\"name\"],\n",
        "        \"learning_rate\": cfg[\"learning_rate\"],\n",
        "        \"eval_loss\": eval_loss,\n",
        "        \"perplexity\": ppl,\n",
        "        \"seconds\": round(secs, 2),\n",
        "    }\n",
        "\n",
        "if RUN_SWEEP:\n",
        "    for cfg in SWEEP_CONFIGS:\n",
        "        sweep_results.append(run_quick_trial(cfg))\n",
        "\n",
        "    # Save sweep results\n",
        "    with open(RUN_DIR / \"sweep_results.json\", \"w\") as f:\n",
        "        json.dump(sweep_results, f, indent=2)\n",
        "\n",
        "    df_sweep = pd.DataFrame(sweep_results).sort_values(\"eval_loss\")\n",
        "    display(df_sweep)\n",
        "\n",
        "    # Pick best LR for final training (lowest eval_loss)\n",
        "    BEST_LR = float(df_sweep.iloc[0][\"learning_rate\"])\n",
        "else:\n",
        "    BEST_LR = 2e-4  # your original setting\n",
        "\n",
        "print(\"BEST_LR selected for final run:\", BEST_LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dfbbcb8f-c359-404b-bca4-2274bca1ef87",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "common_args = dict(\n",
        "    output_dir=\"./gpt2-lyrics-lora\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=float(BEST_LR),  # from quick sweep above\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,  # required regularization\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    logging_steps=20,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# transformers version compatibility:\n",
        "# - older versions use evaluation_strategy\n",
        "# - newer versions use eval_strategy\n",
        "try:\n",
        "    training_args = TrainingArguments(evaluation_strategy=\"epoch\", **common_args)\n",
        "except TypeError:\n",
        "    training_args = TrainingArguments(eval_strategy=\"epoch\", **common_args)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cdbced9e-d8d7-4b49-bc8a-6844a9ad9774",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shweiss\\AppData\\Local\\Temp\\ipykernel_37192\\2783257490.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fcc80b7a-a513-47fb-8117-ceb5f318d792",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [285/285 1:40:51, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.683600</td>\n",
              "      <td>3.361988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.595900</td>\n",
              "      <td>3.334132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.544300</td>\n",
              "      <td>3.319340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.541700</td>\n",
              "      <td>3.313390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.528500</td>\n",
              "      <td>3.311978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:51]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval metrics: {'eval_loss': 3.3119781017303467, 'eval_runtime': 54.1203, 'eval_samples_per_second': 1.848, 'eval_steps_per_second': 0.462, 'epoch': 5.0}\n",
            "Perplexity: 27.43934964877291\n",
            "Training seconds: 6075.38\n",
            "Saved: C:\\Users\\shweiss\\Downloads\\artifacts\\20260221_113232\\run_metrics.json\n",
            "Saved: C:\\Users\\shweiss\\Downloads\\artifacts\\20260221_113232\\trainer_log_history.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_start = time.time()\n",
        "train_output = trainer.train()\n",
        "train_seconds = time.time() - train_start\n",
        "\n",
        "metrics = trainer.evaluate()\n",
        "eval_loss = float(metrics.get(\"eval_loss\", float(\"nan\")))\n",
        "perplexity = float(math.exp(eval_loss)) if eval_loss == eval_loss else float(\"nan\")\n",
        "\n",
        "print(\"Eval metrics:\", metrics)\n",
        "print(\"Perplexity:\", perplexity)\n",
        "print(\"Training seconds:\", round(train_seconds, 2))\n",
        "\n",
        "# Save logs + metrics for submission\n",
        "run_summary = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"timestamp_local\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "    \"base_model\": \"gpt2\",\n",
        "    \"dataset\": \"halaction/song-lyrics (train[:1000])\",\n",
        "    \"seed\": SEED,\n",
        "    \"train_rows\": len(train_dataset),\n",
        "    \"eval_rows\": len(eval_dataset),\n",
        "    \"max_length\": max_length,\n",
        "    \"lora_config\": {\n",
        "        \"r\": lora_config.r,\n",
        "        \"lora_alpha\": lora_config.lora_alpha,\n",
        "        \"lora_dropout\": lora_config.lora_dropout,\n",
        "        \"target_modules\": list(lora_config.target_modules),\n",
        "    },\n",
        "    \"training_args\": training_args.to_dict(),\n",
        "    \"eval_metrics\": metrics,\n",
        "    \"perplexity\": perplexity,\n",
        "    \"train_seconds\": round(train_seconds, 2),\n",
        "}\n",
        "\n",
        "with open(RUN_DIR / \"run_metrics.json\", \"w\") as f:\n",
        "    json.dump(run_summary, f, indent=2)\n",
        "\n",
        "with open(RUN_DIR / \"trainer_log_history.json\", \"w\") as f:\n",
        "    json.dump(trainer.state.log_history, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", (RUN_DIR / \"run_metrics.json\").resolve())\n",
        "print(\"Saved:\", (RUN_DIR / \"trainer_log_history.json\").resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Save artifacts and export to ONNX\nSave the LoRA adapter + merged model, then export to ONNX for lightweight inference.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1666929e-a75d-40f9-9d15-26db933a5326",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapter: C:\\Users\\shweiss\\Downloads\\gpt2-lyrics-lora-adapter\n",
            "Saved merged model: C:\\Users\\shweiss\\Downloads\\gpt2-lyrics-merged\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ADAPTER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save LoRA adapter\n",
        "trainer.model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "\n",
        "# Merge LoRA into base weights for easier inference + ONNX export\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(MERGED_DIR)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "\n",
        "# Copy paths into run folder for neat submission packaging\n",
        "with open(RUN_DIR / \"artifact_paths.json\", \"w\") as f:\n",
        "    json.dump(\n",
        "        {\n",
        "            \"adapter_dir\": str(ADAPTER_DIR.resolve()),\n",
        "            \"merged_dir\": str(MERGED_DIR.resolve()),\n",
        "        },\n",
        "        f,\n",
        "        indent=2,\n",
        "    )\n",
        "\n",
        "print(\"Saved adapter:\", ADAPTER_DIR.resolve())\n",
        "print(\"Saved merged model:\", MERGED_DIR.resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d8381a4a-d65a-4eec-8c0d-0ea1d715880e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\transformers\\cache_utils.py:132: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if not self.is_initialized or self.keys.numel() == 0:\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\transformers\\masking_utils.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\transformers\\masking_utils.py:235: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n",
            "c:\\Users\\shweiss\\AppData\\Local\\anaconda3\\envs\\textml\\Lib\\site-packages\\torch\\onnx\\_internal\\torchscript_exporter\\symbolic_opset11.py:954: UserWarning: Exporting aten::index operator of advanced indexing in opset 18 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  return opset9.index(g, self, index)\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_3300'}\n",
            "\ttransformer.wte.weight: {'transformer.wte.weight'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX export complete: C:\\Users\\shweiss\\Downloads\\gpt2-lyrics-onnx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "ONNX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export merged PyTorch model -> ONNX + load as ORT model\n",
        "ort_model = ORTModelForCausalLM.from_pretrained(str(MERGED_DIR), export=True)\n",
        "ort_model.save_pretrained(ONNX_DIR)\n",
        "tokenizer.save_pretrained(ONNX_DIR)\n",
        "\n",
        "# Record ONNX artifact path\n",
        "with open(RUN_DIR / \"onnx_export.json\", \"w\") as f:\n",
        "    json.dump({\"onnx_dir\": str(ONNX_DIR.resolve())}, f, indent=2)\n",
        "\n",
        "print(\"ONNX export complete:\", ONNX_DIR.resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a84288d5-4c58-4ff9-8607-f8ce10d72851",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX_AVAILABLE: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Load PyTorch merged model for fallback\n",
        "pt_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pt_model = AutoModelForCausalLM.from_pretrained(MERGED_DIR).to(pt_device)\n",
        "pt_model.eval()\n",
        "\n",
        "# Load ONNX Runtime model if present\n",
        "try:\n",
        "    from optimum.onnxruntime import ORTModelForCausalLM\n",
        "    ort_model = ORTModelForCausalLM.from_pretrained(ONNX_DIR)\n",
        "    ONNX_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    ort_model = None\n",
        "    ONNX_AVAILABLE = False\n",
        "    print(\"ONNX model not available yet (run export cell first). Details:\", e)\n",
        "\n",
        "def _generate_with_model(model_obj, prompt, max_new_tokens=80, temperature=0.9, top_p=0.95):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs.get(\"attention_mask\", None)\n",
        "\n",
        "    # ORT models run on CPU by default; PyTorch model uses pt_device\n",
        "    if isinstance(model_obj, torch.nn.Module):\n",
        "        input_ids = input_ids.to(pt_device)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(pt_device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_ids = model_obj.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=int(max_new_tokens),\n",
        "            do_sample=True,\n",
        "            temperature=float(temperature),\n",
        "            top_p=float(top_p),\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def generate_lyrics(prompt, backend=\"onnx\", max_new_tokens=80, temperature=0.9, top_p=0.95):\n",
        "    backend = (backend or \"onnx\").lower()\n",
        "    if backend == \"onnx\" and ONNX_AVAILABLE:\n",
        "        return _generate_with_model(ort_model, prompt, max_new_tokens, temperature, top_p)\n",
        "    return _generate_with_model(pt_model, prompt, max_new_tokens, temperature, top_p)\n",
        "\n",
        "print(\"ONNX_AVAILABLE:\", ONNX_AVAILABLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Local deployment with Gradio\nRun a simple local UI to generate lyrics from prompts (PyTorch fallback, ONNX when available).\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e51955eb-c161-46cc-8958-4dcfc90af1e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching Gradio on: http://127.0.0.1:54345/  (leave this cell running)\n",
            "* Running on local URL:  http://127.0.0.1:54345\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:54345/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "# Ensure localhost bypasses proxies (helps in some environments)\n",
        "for k in [\"NO_PROXY\", \"no_proxy\"]:\n",
        "    cur = os.environ.get(k, \"\")\n",
        "    add = \"127.0.0.1,localhost\"\n",
        "    if add not in cur:\n",
        "        os.environ[k] = (cur + \",\" if cur else \"\") + add\n",
        "\n",
        "def gr_generate(backend, prompt, max_new_tokens, temperature, top_p):\n",
        "    prompt = (prompt or \"\").strip()\n",
        "    if not prompt:\n",
        "        return \"Please enter a prompt (even a short phrase).\"\n",
        "    return generate_lyrics(\n",
        "        prompt=prompt,\n",
        "        backend=backend,\n",
        "        max_new_tokens=int(max_new_tokens),\n",
        "        temperature=float(temperature),\n",
        "        top_p=float(top_p),\n",
        "    )\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gr_generate,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=[\"onnx\", \"pytorch\"], value=\"onnx\", label=\"Backend\"),\n",
        "        gr.Textbox(lines=3, label=\"Prompt\", placeholder=\"Type a verse starter, hook, or first line...\"),\n",
        "        gr.Slider(20, 200, value=80, step=1, label=\"max_new_tokens\"),\n",
        "        gr.Slider(0.1, 1.5, value=0.9, step=0.05, label=\"temperature\"),\n",
        "        gr.Slider(0.5, 1.0, value=0.95, step=0.01, label=\"top_p\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(lines=12, label=\"Generated Lyrics\"),\n",
        "    title=\"GPT-2 Lyrics Generator (LoRA Fine-Tuned)\",\n",
        "    description=\"Runs locally. ONNX backend is recommended when available; PyTorch is the fallback.\",\n",
        "    flagging_mode=\"never\",\n",
        ")\n",
        "\n",
        "PORT = 54345\n",
        "print(f\"Launching Gradio on: http://127.0.0.1:{PORT}/  (leave this cell running)\")\n",
        "demo.launch(server_name=\"127.0.0.1\", server_port=PORT, share=False, show_error=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Evaluation: BLEU + qualitative review\nCompute BLEU on held-out continuations and document qualitative observations.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "22c53034-a9d6-4acd-9371-3beab2f54ef6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU (held-out continuation): 0.004015177173907737\n",
            "Saved: C:\\Users\\shweiss\\Downloads\\artifacts\\manual_run\\bleu_samples.json\n",
            "\n",
            "Qualitative checklist:\n",
            "- Coherence: does it stay on a consistent theme?\n",
            "- Relevance: does it continue the prompt naturally?\n",
            "- Creativity: imagery and phrasing variety?\n",
            "- Fluency: grammar/readability?\n",
            "- Repetition: does it loop? If yes, adjust top_p, temperature, repetition_penalty.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def split_prompt_reference(text, prompt_words=18, ref_max_words=80):\n",
        "    words = (text or \"\").split()\n",
        "    if len(words) < prompt_words + 5:\n",
        "        prompt = \" \".join(words[: max(5, len(words)//2)])\n",
        "        ref = \" \".join(words[len(prompt.split()):])\n",
        "        return prompt, ref\n",
        "    prompt = \" \".join(words[:prompt_words])\n",
        "    ref = \" \".join(words[prompt_words:prompt_words + ref_max_words])\n",
        "    return prompt, ref\n",
        "\n",
        "NUM_BLEU_SAMPLES = 10\n",
        "PROMPT_WORDS = 18\n",
        "\n",
        "# Robust raw_text retrieval even if you used set_format(type=\"torch\", ...)\n",
        "try:\n",
        "    eval_dataset.reset_format()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if \"raw_text\" in getattr(eval_dataset, \"column_names\", []):\n",
        "    eval_texts = eval_dataset.data.column(\"raw_text\").to_pylist()\n",
        "else:\n",
        "    eval_texts = []\n",
        "\n",
        "samples = []\n",
        "bleu_scores = []\n",
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "for i in range(min(NUM_BLEU_SAMPLES, len(eval_texts))):\n",
        "    text = eval_texts[i]\n",
        "    prompt, reference_text = split_prompt_reference(text, prompt_words=PROMPT_WORDS)\n",
        "\n",
        "    if not reference_text.strip():\n",
        "        continue\n",
        "\n",
        "    generated_full = generate_lyrics(prompt, backend=\"onnx\", max_new_tokens=60)\n",
        "\n",
        "    if generated_full.lower().startswith(prompt.lower()):\n",
        "        continuation = generated_full[len(prompt):].strip()\n",
        "    else:\n",
        "        continuation = generated_full.strip()\n",
        "\n",
        "    reference_tokens = [reference_text.split()]\n",
        "    candidate_tokens = continuation.split()\n",
        "\n",
        "    bleu = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smooth)\n",
        "    bleu_scores.append(float(bleu))\n",
        "\n",
        "    samples.append(\n",
        "        {\n",
        "            \"i\": i,\n",
        "            \"prompt\": prompt,\n",
        "            \"reference_continuation\": reference_text,\n",
        "            \"generated_full\": generated_full,\n",
        "            \"generated_continuation\": continuation,\n",
        "            \"bleu\": float(bleu),\n",
        "        }\n",
        "    )\n",
        "\n",
        "avg_bleu = float(sum(bleu_scores) / len(bleu_scores)) if bleu_scores else float(\"nan\")\n",
        "print(\"Average BLEU (held-out continuation):\", avg_bleu)\n",
        "\n",
        "with open(RUN_DIR / \"bleu_samples.json\", \"w\") as f:\n",
        "    json.dump({\"avg_bleu\": avg_bleu, \"n\": len(samples), \"samples\": samples}, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", (RUN_DIR / \"bleu_samples.json\").resolve())\n",
        "\n",
        "print(\"\\nQualitative checklist:\")\n",
        "print(\"- Coherence: does it stay on a consistent theme?\")\n",
        "print(\"- Relevance: does it continue the prompt naturally?\")\n",
        "print(\"- Creativity: imagery and phrasing variety?\")\n",
        "print(\"- Fluency: grammar/readability?\")\n",
        "print(\"- Repetition: does it loop? If yes, adjust top_p, temperature, repetition_penalty.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "textml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}