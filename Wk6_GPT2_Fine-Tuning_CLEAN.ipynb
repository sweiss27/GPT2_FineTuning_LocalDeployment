{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7f1880",
   "metadata": {},
   "source": [
    "# Week 6 — GPT‑2 Lyrics Fine‑Tuning (LoRA) + ONNX + Gradio\n",
    "\n",
    "This notebook is organized to run top‑to‑bottom in VS Code notebooks or Jupyter.\n",
    "\n",
    "**Outputs created** (for submission):\n",
    "- `artifacts/<run_id>/run_metrics.json`\n",
    "- `artifacts/<run_id>/trainer_log_history.json`\n",
    "- `artifacts/<run_id>/sweep_results.json` (minimal tuning evidence)\n",
    "- `artifacts/<run_id>/onnx_export.json`\n",
    "- `artifacts/<run_id>/artifact_paths.json`\n",
    "- `artifacts/<run_id>/bleu_samples.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41110aa1",
   "metadata": {},
   "source": [
    "## 0) Install dependencies\n",
    "Assignment note: the course instructions show `!pip install ...`. If you are in VS Code notebooks, `%pip install ...` also works and installs into the active kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you prefer, replace !pip with %pip in VS Code notebooks\n",
    "!pip -q install datasets transformers peft accelerate evaluate nltk onnx onnxruntime gradio optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fee0fa",
   "metadata": {},
   "source": [
    "## 1) Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc089d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, random, socket\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Dataset + tokenization\n",
    "DATASET_ID = \"halaction/song-lyrics\"\n",
    "SUBSET = \"train[:1000]\"      # required subset used in your run\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Output folders\n",
    "ARTIFACTS_DIR = Path(\"./artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "RUN_ID = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = ARTIFACTS_DIR / RUN_ID\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ADAPTER_DIR = Path(\"./gpt2-lyrics-lora-adapter\")\n",
    "MERGED_DIR  = Path(\"./gpt2-lyrics-merged\")\n",
    "ONNX_DIR    = Path(\"./gpt2-lyrics-onnx\")\n",
    "\n",
    "print(\"RUN_DIR:\", RUN_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38c6b8",
   "metadata": {},
   "source": [
    "## 2) Load dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07506e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_ID, split=SUBSET)\n",
    "print(\"Columns:\", dataset.column_names)\n",
    "print(\"Rows:\", len(dataset))\n",
    "\n",
    "# Pick a likely text column and clean empties\n",
    "candidate_columns = [\"lyrics\", \"text\", \"song\", \"content\"]\n",
    "text_col = next((c for c in candidate_columns if c in dataset.column_names), dataset.column_names[0])\n",
    "\n",
    "dataset = dataset.filter(lambda x: x.get(text_col) is not None and str(x.get(text_col)).strip() != \"\")\n",
    "dataset = dataset.select_columns([text_col])\n",
    "\n",
    "print(\"Using text column:\", text_col)\n",
    "print(\"Rows after cleaning:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0fc2a7",
   "metadata": {},
   "source": [
    "## 3) Tokenizer + tokenization\n",
    "Important details:\n",
    "- GPT‑2 has no pad token by default, so we set `pad_token = eos_token`.\n",
    "- We keep `raw_text` so BLEU can read held‑out text even if we later set the dataset format to torch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca037a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    raw_texts = batch[text_col]\n",
    "    # Add EOS so generations learn an end-of-sequence signal\n",
    "    texts = [str(t) + tokenizer.eos_token for t in raw_texts]\n",
    "    tok = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    tok[\"labels\"] = tok[\"input_ids\"].copy()\n",
    "    tok[\"raw_text\"] = raw_texts\n",
    "    return tok\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[text_col])\n",
    "splits = tokenized.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_dataset = splits[\"train\"]\n",
    "eval_dataset  = splits[\"test\"]\n",
    "\n",
    "# Set torch format for training columns (raw_text stays in Arrow storage)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\"Train rows:\", len(train_dataset), \"| Eval rows:\", len(eval_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cdf7c",
   "metadata": {},
   "source": [
    "## 4) Load GPT‑2 and apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03681d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f667e0",
   "metadata": {},
   "source": [
    "## 5) Training setup (weight decay + early stopping)\n",
    "This matches your run settings (effective batch size 16, LR 2e‑4, weight decay 0.01, 5 epochs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(ADAPTER_DIR),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # effective batch ~ 16\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566b09c",
   "metadata": {},
   "source": [
    "## 6) Minimal hyperparameter tuning evidence (quick LR sweep)\n",
    "This runs short trials to compare two learning rates. If you already ran this once, you can skip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d958db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_lr_trial(lr, max_steps=150):\n",
    "    tmp_args = TrainingArguments(\n",
    "        output_dir=str(RUN_DIR / f\"trial_lr_{lr}\"),\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        max_steps=max_steps,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=max_steps,\n",
    "        save_strategy=\"no\",\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=[],\n",
    "    )\n",
    "    # fresh model for fair comparison\n",
    "    bm = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    bm.resize_token_embeddings(len(tokenizer))\n",
    "    m = get_peft_model(bm, lora_cfg)\n",
    "\n",
    "    t = Trainer(\n",
    "        model=m,\n",
    "        args=tmp_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    t0 = time.time()\n",
    "    t.train()\n",
    "    metrics = t.evaluate()\n",
    "    secs = time.time() - t0\n",
    "\n",
    "    ppl = float(math.exp(metrics[\"eval_loss\"])) if metrics.get(\"eval_loss\") else float(\"nan\")\n",
    "    return {\"learning_rate\": lr, \"eval_loss\": float(metrics[\"eval_loss\"]), \"perplexity\": ppl, \"seconds\": round(secs, 2)}\n",
    "\n",
    "sweep_results = [\n",
    "    {\"name\":\"lr_2e-4\", **quick_lr_trial(2e-4, max_steps=150)},\n",
    "    {\"name\":\"lr_1e-4\", **quick_lr_trial(1e-4, max_steps=150)},\n",
    "]\n",
    "\n",
    "with open(RUN_DIR / \"sweep_results.json\", \"w\") as f:\n",
    "    json.dump(sweep_results, f, indent=2)\n",
    "\n",
    "sweep_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800454d",
   "metadata": {},
   "source": [
    "## 7) Train the final LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "train_output = trainer.train()\n",
    "train_seconds = time.time() - t0\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "perplexity = float(math.exp(eval_metrics[\"eval_loss\"]))\n",
    "\n",
    "print(\"Eval loss:\", eval_metrics[\"eval_loss\"])\n",
    "print(\"Perplexity:\", perplexity)\n",
    "print(\"Train seconds:\", round(train_seconds, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5135512",
   "metadata": {},
   "source": [
    "## 8) Save adapter, merge weights, export ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1016eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "trainer.model.save_pretrained(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_DIR)\n",
    "\n",
    "# Merge LoRA into base model and save a standard Transformers folder\n",
    "merged_model = trainer.model.merge_and_unload()\n",
    "MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "merged_model.save_pretrained(MERGED_DIR)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "artifact_paths = {\n",
    "    \"adapter_dir\": str(ADAPTER_DIR.resolve()),\n",
    "    \"merged_dir\": str(MERGED_DIR.resolve()),\n",
    "}\n",
    "with open(RUN_DIR / \"artifact_paths.json\", \"w\") as f:\n",
    "    json.dump(artifact_paths, f, indent=2)\n",
    "\n",
    "print(\"Saved adapter + merged model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27267a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX (Optimum ONNX Runtime)\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "\n",
    "ONNX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "onnx_model = ORTModelForCausalLM.from_pretrained(MERGED_DIR, export=True)\n",
    "onnx_model.save_pretrained(ONNX_DIR)\n",
    "tokenizer.save_pretrained(ONNX_DIR)\n",
    "\n",
    "onnx_export = {\"onnx_dir\": str(ONNX_DIR.resolve())}\n",
    "with open(RUN_DIR / \"onnx_export.json\", \"w\") as f:\n",
    "    json.dump(onnx_export, f, indent=2)\n",
    "\n",
    "print(\"ONNX saved to:\", ONNX_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865f792",
   "metadata": {},
   "source": [
    "## 9) Generation helper (ONNX preferred, PyTorch fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb659248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "_tok = None\n",
    "_pt_model = None\n",
    "_onnx_model = None\n",
    "\n",
    "def _get_tokenizer():\n",
    "    global _tok\n",
    "    if _tok is None:\n",
    "        try:\n",
    "            _tok = AutoTokenizer.from_pretrained(ONNX_DIR)\n",
    "        except Exception:\n",
    "            _tok = AutoTokenizer.from_pretrained(MERGED_DIR)\n",
    "        if _tok.pad_token is None:\n",
    "            _tok.pad_token = _tok.eos_token\n",
    "    return _tok\n",
    "\n",
    "def _get_pt_model():\n",
    "    global _pt_model\n",
    "    if _pt_model is None:\n",
    "        _pt_model = AutoModelForCausalLM.from_pretrained(MERGED_DIR)\n",
    "        _pt_model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            _pt_model.to(\"cuda\")\n",
    "    return _pt_model\n",
    "\n",
    "def _get_onnx_model():\n",
    "    global _onnx_model\n",
    "    if _onnx_model is None:\n",
    "        from optimum.onnxruntime import ORTModelForCausalLM\n",
    "        _onnx_model = ORTModelForCausalLM.from_pretrained(ONNX_DIR)\n",
    "    return _onnx_model\n",
    "\n",
    "def generate_lyrics(prompt, backend=\"onnx\", max_new_tokens=60, temperature=0.9, top_p=0.95, repetition_penalty=1.1):\n",
    "    tok = _get_tokenizer()\n",
    "    inputs = tok(prompt, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    if backend == \"pytorch\":\n",
    "        model = _get_pt_model()\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=int(max_new_tokens),\n",
    "                do_sample=True,\n",
    "                temperature=float(temperature),\n",
    "                top_p=float(top_p),\n",
    "                repetition_penalty=float(repetition_penalty),\n",
    "                pad_token_id=tok.eos_token_id,\n",
    "            )\n",
    "        return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        model = _get_onnx_model()\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "            do_sample=True,\n",
    "            temperature=float(temperature),\n",
    "            top_p=float(top_p),\n",
    "            repetition_penalty=float(repetition_penalty),\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "        return tok.decode(out[0], skip_special_tokens=True)\n",
    "    except Exception:\n",
    "        return generate_lyrics(prompt, backend=\"pytorch\", max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "\n",
    "print(generate_lyrics(\"late night drive, neon lights\", backend=\"pytorch\", max_new_tokens=40)[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f9d06",
   "metadata": {},
   "source": [
    "## 10) BLEU evaluation (multiple samples)\n",
    "BLEU is a lightweight sanity check here. Lyric generation is open-ended, so exact n‑gram overlap with a single reference is rare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da55c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def split_prompt_reference(text, prompt_words=18, ref_max_words=80):\n",
    "    words = (text or \"\").split()\n",
    "    if len(words) < prompt_words + 5:\n",
    "        prompt = \" \".join(words[: max(5, len(words)//2)])\n",
    "        ref = \" \".join(words[len(prompt.split()):])\n",
    "        return prompt, ref\n",
    "    prompt = \" \".join(words[:prompt_words])\n",
    "    ref = \" \".join(words[prompt_words:prompt_words + ref_max_words])\n",
    "    return prompt, ref\n",
    "\n",
    "NUM_BLEU_SAMPLES = 10\n",
    "PROMPT_WORDS = 18\n",
    "\n",
    "# Read raw text from Arrow storage (safe even if set_format was used)\n",
    "eval_dataset.reset_format()\n",
    "eval_texts = eval_dataset.data.column(\"raw_text\").to_pylist() if \"raw_text\" in eval_dataset.column_names else []\n",
    "\n",
    "samples = []\n",
    "bleu_scores = []\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "for i in range(min(NUM_BLEU_SAMPLES, len(eval_texts))):\n",
    "    text = eval_texts[i]\n",
    "    prompt, reference_text = split_prompt_reference(text, prompt_words=PROMPT_WORDS)\n",
    "    if not reference_text.strip():\n",
    "        continue\n",
    "\n",
    "    generated_full = generate_lyrics(prompt, backend=\"onnx\", max_new_tokens=60)\n",
    "\n",
    "    continuation = generated_full[len(prompt):].strip() if generated_full.lower().startswith(prompt.lower()) else generated_full.strip()\n",
    "    reference_tokens = [reference_text.split()]\n",
    "    candidate_tokens = continuation.split()\n",
    "\n",
    "    bleu = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smooth)\n",
    "    bleu_scores.append(float(bleu))\n",
    "\n",
    "    samples.append({\n",
    "        \"i\": i,\n",
    "        \"prompt\": prompt,\n",
    "        \"reference_continuation\": reference_text,\n",
    "        \"generated_full\": generated_full,\n",
    "        \"generated_continuation\": continuation,\n",
    "        \"bleu\": float(bleu),\n",
    "    })\n",
    "\n",
    "avg_bleu = float(sum(bleu_scores) / len(bleu_scores)) if bleu_scores else float(\"nan\")\n",
    "print(\"Average BLEU:\", avg_bleu)\n",
    "\n",
    "with open(RUN_DIR / \"bleu_samples.json\", \"w\") as f:\n",
    "    json.dump({\"avg_bleu\": avg_bleu, \"n\": len(samples), \"samples\": samples}, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", (RUN_DIR / \"bleu_samples.json\").resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfc0a1",
   "metadata": {},
   "source": [
    "## 11) Save final run metrics + logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"timestamp_local\": time.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "    \"base_model\": \"gpt2\",\n",
    "    \"dataset\": f\"{DATASET_ID} ({SUBSET})\",\n",
    "    \"seed\": SEED,\n",
    "    \"train_rows\": len(train_dataset),\n",
    "    \"eval_rows\": len(eval_dataset),\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_cfg.r,\n",
    "        \"lora_alpha\": lora_cfg.lora_alpha,\n",
    "        \"lora_dropout\": lora_cfg.lora_dropout,\n",
    "        \"target_modules\": lora_cfg.target_modules,\n",
    "    },\n",
    "    \"training_args\": training_args.to_dict(),\n",
    "    \"eval_metrics\": {k: float(v) for k, v in eval_metrics.items() if isinstance(v, (int, float))},\n",
    "    \"perplexity\": perplexity,\n",
    "    \"train_seconds\": round(train_seconds, 2),\n",
    "}\n",
    "\n",
    "with open(RUN_DIR / \"run_metrics.json\", \"w\") as f:\n",
    "    json.dump(run_metrics, f, indent=2)\n",
    "\n",
    "with open(RUN_DIR / \"trainer_log_history.json\", \"w\") as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "print(\"Saved run_metrics.json and trainer_log_history.json to:\", RUN_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c43839",
   "metadata": {},
   "source": [
    "## 12) Gradio app (local deployment)\n",
    "VS Code notebooks note: older Gradio versions do not support `block=`. This launch code avoids port conflicts and works locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd900e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gr_generate(backend, prompt, max_new_tokens, temperature, top_p):\n",
    "    return generate_lyrics(\n",
    "        prompt=prompt,\n",
    "        backend=backend,\n",
    "        max_new_tokens=int(max_new_tokens),\n",
    "        temperature=float(temperature),\n",
    "        top_p=float(top_p),\n",
    "    )\n",
    "\n",
    "try:\n",
    "    demo = gr.Interface(\n",
    "        fn=gr_generate,\n",
    "        inputs=[\n",
    "            gr.Dropdown(choices=[\"onnx\", \"pytorch\"], value=\"onnx\", label=\"Backend\"),\n",
    "            gr.Textbox(lines=2, label=\"Prompt\"),\n",
    "            gr.Slider(20, 200, value=80, step=1, label=\"max_new_tokens\"),\n",
    "            gr.Slider(0.1, 1.5, value=0.9, step=0.05, label=\"temperature\"),\n",
    "            gr.Slider(0.5, 1.0, value=0.95, step=0.01, label=\"top_p\"),\n",
    "        ],\n",
    "        outputs=gr.Textbox(lines=10, label=\"Generated Lyrics\"),\n",
    "        title=\"GPT-2 Lyrics Generator (LoRA Fine-Tuned)\",\n",
    "        description=\"Runs locally. ONNX backend is recommended when available; PyTorch is the fallback.\",\n",
    "        flagging_mode=\"never\",\n",
    "    )\n",
    "except TypeError:\n",
    "    demo = gr.Interface(\n",
    "        fn=gr_generate,\n",
    "        inputs=[\n",
    "            gr.Dropdown(choices=[\"onnx\", \"pytorch\"], value=\"onnx\", label=\"Backend\"),\n",
    "            gr.Textbox(lines=2, label=\"Prompt\"),\n",
    "            gr.Slider(20, 200, value=80, step=1, label=\"max_new_tokens\"),\n",
    "            gr.Slider(0.1, 1.5, value=0.9, step=0.05, label=\"temperature\"),\n",
    "            gr.Slider(0.5, 1.0, value=0.95, step=0.01, label=\"top_p\"),\n",
    "        ],\n",
    "        outputs=gr.Textbox(lines=10, label=\"Generated Lyrics\"),\n",
    "        title=\"GPT-2 Lyrics Generator (LoRA Fine-Tuned)\",\n",
    "        description=\"Runs locally. ONNX backend is recommended when available; PyTorch is the fallback.\",\n",
    "        allow_flagging=\"never\",\n",
    "    )\n",
    "\n",
    "# Avoid proxy surprises for localhost\n",
    "for k in [\"HTTP_PROXY\",\"HTTPS_PROXY\",\"ALL_PROXY\",\"http_proxy\",\"https_proxy\",\"all_proxy\"]:\n",
    "    os.environ.pop(k, None)\n",
    "os.environ[\"NO_PROXY\"] = \"127.0.0.1,localhost\"\n",
    "os.environ[\"no_proxy\"] = \"127.0.0.1,localhost\"\n",
    "\n",
    "# Pick an open port (Windows safe)\n",
    "sock = socket.socket()\n",
    "sock.bind((\"127.0.0.1\", 0))\n",
    "port = sock.getsockname()[1]\n",
    "sock.close()\n",
    "\n",
    "print(\"Launching Gradio on port:\", port)\n",
    "demo.launch(server_name=\"127.0.0.1\", server_port=port, prevent_thread_lock=True)\n",
    "print(f\"Open: http://127.0.0.1:{port}/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}