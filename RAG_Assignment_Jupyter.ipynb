{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Assignment: Simple RAG System (Jupyter + Python)\n\nThis notebook gives you **clean, copy-ready code** for:\n1. Data preprocessing and chunking\n2. FAISS retrieval with TF-IDF embeddings\n3. GPT-2 response generation from retrieved context\n4. Basic evaluation (relevance, completeness, coherence)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0) Install libraries (run once)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q datasets faiss-cpu scikit-learn transformers torch"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Imports + configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport numpy as np\nimport torch\nimport faiss\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", DEVICE)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Load a small custom dataset (Wikipedia subset)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Streaming keeps memory usage low\ndataset_stream = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True)\narticles = list(dataset_stream.take(250))\nprint(\"Articles loaded:\", len(articles))\nprint(\"Example title:\", articles[0].get(\"title\", \"N/A\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Preprocess + chunk text"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def clean_text(text: str) -> str:\n    \"\"\"Lowercase and remove noisy characters while preserving spaces.\"\"\"\n    text = text.lower()\n    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\ndef chunk_text(text: str, chunk_size: int = 120, overlap: int = 20):\n    \"\"\"Split text into overlapping word chunks for better retrieval.\"\"\"\n    words = clean_text(text).split()\n    if not words:\n        return []\n\n    step = max(1, chunk_size - overlap)\n    chunks = []\n    for i in range(0, len(words), step):\n        chunk_words = words[i:i + chunk_size]\n        if len(chunk_words) >= 25:  # filter tiny chunks\n            chunks.append(\" \".join(chunk_words))\n    return chunks\n\n\nall_chunks = []\nfor item in articles:\n    all_chunks.extend(chunk_text(item.get(\"text\", \"\"), chunk_size=120, overlap=20))\n\nprint(\"Total chunks created:\", len(all_chunks))\nprint(\"Sample chunk:\n\", all_chunks[0][:350])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) Build retrieval index (TF-IDF + FAISS)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Vectorize chunks with TF-IDF\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nX = vectorizer.fit_transform(all_chunks)\nchunk_vectors = X.toarray().astype(np.float32)\n\n# Normalize to use inner product as cosine similarity\nfaiss.normalize_L2(chunk_vectors)\n\n# Build FAISS index\nindex = faiss.IndexFlatIP(chunk_vectors.shape[1])\nindex.add(chunk_vectors)\n\nprint(\"TF-IDF shape:\", chunk_vectors.shape)\nprint(\"Indexed vectors:\", index.ntotal)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) Query and retrieve relevant chunks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def retrieve(query: str, k: int = 3):\n    q_vec = vectorizer.transform([clean_text(query)]).toarray().astype(np.float32)\n    faiss.normalize_L2(q_vec)\n    scores, ids = index.search(q_vec, k)\n\n    results = []\n    for score, idx in zip(scores[0], ids[0]):\n        results.append({\n            \"chunk_id\": int(idx),\n            \"score\": float(score),\n            \"text\": all_chunks[idx]\n        })\n    return results\n\n\nuser_query = \"What is the capital of France?\"\nretrieved_docs = retrieve(user_query, k=3)\n\nfor i, doc in enumerate(retrieved_docs, start=1):\n    print(f\"Top {i} | score={doc['score']:.4f}\")\n    print(doc['text'][:300], \"\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6) Generate answer with GPT-2 using retrieved context"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n\ndef generate_answer(query: str, retrieved_docs, max_new_tokens: int = 120):\n    context = \"\n\".join([f\"- {d['text']}\" for d in retrieved_docs])\n\n    prompt = (\n        \"You are a helpful assistant. Answer the question using only the context.\n\n\"\n        f\"Question: {query}\n\n\"\n        f\"Context:\n{context}\n\n\"\n        \"Answer:\"\n    )\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=900).to(DEVICE)\n\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            no_repeat_ngram_size=2\n        )\n\n    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return generated.split(\"Answer:\")[-1].strip()\n\n\nanswer = generate_answer(user_query, retrieved_docs)\nprint(\"Query:\", user_query)\nprint(\"Generated Answer:\n\", answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7) Evaluation (required metrics)\n\nGiven corpus/query/response, compute:\n- **Relevance** = cosine similarity\n- **Completeness** = proportion of query keywords found in response\n- **Coherence** = GPT-2 perplexity (lower is better)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "corpus = [\n    \"Distributed Data Parallel (DDP) allows multi-GPU training.\",\n    \"DDP synchronizes gradients across GPUs.\",\n    \"Using multiple GPUs increases training efficiency.\",\n    \"Parallelization across GPUs is useful for large models.\"\n]\n\nquery = \"What is Distributed Data Parallel (DDP) in PyTorch, and how is it useful in multi-GPU setups?\"\nresponse = \"Distributed Data Parallel (DDP) in PyTorch is a module that enables parallel training across multiple GPUs by distributing model replicas and splitting data across them.\"\n\n# Relevance: cosine similarity between query and response\nrel_vec = TfidfVectorizer(stop_words=\"english\")\nqr = rel_vec.fit_transform([query, response])\nrelevance = cosine_similarity(qr[0:1], qr[1:2])[0, 0]\n\n# Completeness: proportion of important query keywords found in response\ndef keywords(text: str):\n    stop = {\"what\",\"is\",\"in\",\"and\",\"how\",\"it\",\"the\",\"a\",\"an\",\"of\",\"to\",\"for\",\"on\",\"by\",\"with\"}\n    toks = re.findall(r\"[a-zA-Z]+\", text.lower())\n    return sorted(set([t for t in toks if t not in stop and len(t) > 2]))\n\nq_keywords = keywords(query)\nr_tokens = set(re.findall(r\"[a-zA-Z]+\", response.lower()))\ncovered = [k for k in q_keywords if k in r_tokens]\ncompleteness = len(covered) / max(1, len(q_keywords))\n\n# Coherence: perplexity under GPT-2\ndef perplexity(text: str):\n    inp = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(DEVICE)\n    with torch.no_grad():\n        out = model(**inp, labels=inp[\"input_ids\"])\n    return float(torch.exp(out.loss))\n\ncoherence_ppl = perplexity(response)\n\nprint(f\"Relevance (cosine similarity): {relevance:.4f}\")\nprint(f\"Completeness (keyword coverage): {completeness:.4f}\")\nprint(f\"Coherence (perplexity, lower is better): {coherence_ppl:.4f}\")\nprint(\"Query keywords:\", q_keywords)\nprint(\"Covered keywords:\", covered)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8) Optional experiment: compare chunk sizes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_chunk_size(sample_articles, query_text, chunk_size):\n    temp_chunks = []\n    for art in sample_articles:\n        temp_chunks.extend(chunk_text(art.get(\"text\", \"\"), chunk_size=chunk_size, overlap=max(5, chunk_size // 6)))\n\n    temp_vec = TfidfVectorizer(max_features=4000, ngram_range=(1, 2))\n    temp_matrix = temp_vec.fit_transform(temp_chunks).toarray().astype(np.float32)\n    faiss.normalize_L2(temp_matrix)\n\n    temp_index = faiss.IndexFlatIP(temp_matrix.shape[1])\n    temp_index.add(temp_matrix)\n\n    q = temp_vec.transform([clean_text(query_text)]).toarray().astype(np.float32)\n    faiss.normalize_L2(q)\n    scores, ids = temp_index.search(q, 3)\n\n    return {\n        \"chunk_size\": chunk_size,\n        \"num_chunks\": len(temp_chunks),\n        \"top_score\": float(scores[0][0])\n    }\n\nfor cs in [80, 120, 180]:\n    print(evaluate_chunk_size(articles[:120], \"what is france capital city\", cs))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9) Qualitative observations for report/PPT/video\n\nUse this structure in your submission:\n- **Relevance:** Were top chunks directly related to the query?\n- **Coherence:** Was the generated answer clear and logically written?\n- **Completeness:** Did it fully answer the query?\n- **Challenges:** Retrieval misses, GPT-2 hallucination, limited context length.\n- **Improvements:** Better embedding model, reranker, stronger prompt constraints, answer citation checks."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}