{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Simple RAG Assignment (Jupyter, Python Kernel)\n\nThis notebook implements:\n1. Dataset loading + preprocessing + chunking\n2. TF-IDF embeddings + FAISS index\n3. Query retrieval\n4. GPT-2 grounded response generation\n5. Qualitative + lightweight quantitative evaluation\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q datasets faiss-cpu scikit-learn transformers torch nltk"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport numpy as np\nimport torch\nimport faiss\nimport nltk\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\nnltk.download('punkt', quiet=True)\n\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Load, clean, and chunk dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load a small streaming subset of Wikipedia\nstream_ds = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True)\nsubset = list(stream_ds.take(300))  # keep it lightweight for notebook runtime\n\n\ndef clean_text(text: str) -> str:\n    text = text.lower()\n    text = re.sub(r\"\\\\s+\", \" \", text)\n    text = re.sub(r\"[^a-z0-9.,;:!?()'\\\"\\\\- ]+\", \" \", text)\n    return text.strip()\n\n\ndef chunk_text_by_words(text: str, chunk_size: int = 120, overlap: int = 20):\n    words = clean_text(text).split()\n    if not words:\n        return []\n    chunks = []\n    step = max(1, chunk_size - overlap)\n    for start in range(0, len(words), step):\n        chunk = words[start:start + chunk_size]\n        if len(chunk) >= 25:  # ignore tiny fragments\n            chunks.append(\" \".join(chunk))\n    return chunks\n\nall_chunks = []\nfor article in subset:\n    txt = article.get(\"text\", \"\")\n    all_chunks.extend(chunk_text_by_words(txt, chunk_size=120, overlap=20))\n\nprint(f\"Articles loaded: {len(subset)}\")\nprint(f\"Total chunks: {len(all_chunks)}\")\nprint(\"Sample chunk:\\n\", all_chunks[0][:400])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Build retrieval with TF-IDF + FAISS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TF-IDF embeddings\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\ntfidf_matrix = vectorizer.fit_transform(all_chunks)\nchunk_embeddings = tfidf_matrix.toarray().astype(np.float32)\n\n# Normalize vectors and build cosine-sim FAISS via inner product\nfaiss.normalize_L2(chunk_embeddings)\nindex = faiss.IndexFlatIP(chunk_embeddings.shape[1])\nindex.add(chunk_embeddings)\n\nprint(\"Embedding shape:\", chunk_embeddings.shape)\nprint(\"FAISS index size:\", index.ntotal)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Query and retrieve top-k chunks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def retrieve_chunks(query: str, k: int = 3):\n    q_vec = vectorizer.transform([clean_text(query)]).toarray().astype(np.float32)\n    faiss.normalize_L2(q_vec)\n    scores, ids = index.search(q_vec, k)\n    hits = []\n    for score, idx in zip(scores[0], ids[0]):\n        hits.append({\n            \"score\": float(score),\n            \"chunk\": all_chunks[idx],\n            \"chunk_id\": int(idx)\n        })\n    return hits\n\nquery = \"What is the capital of France?\"\nretrieved = retrieve_chunks(query, k=3)\n\nfor i, hit in enumerate(retrieved, 1):\n    print(f\"Top {i} | score={hit['score']:.4f}\")\n    print(hit['chunk'][:300], \"\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Generate response using GPT-2 with retrieved context"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# GPT-2 has no pad token by default; map it to eos\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = tokenizer.eos_token_id\n\n\ndef generate_answer_from_chunks(user_query: str, retrieved_chunks, max_new_tokens: int = 120):\n    context = \"\\n\".join([f\"- {x['chunk']}\" if isinstance(x, dict) else f\"- {x}\" for x in retrieved_chunks])\n    prompt = (\n        \"You are a helpful assistant. Use ONLY the context to answer the question.\\n\\n\"\n        f\"Question: {user_query}\\n\\n\"\n        f\"Context:\\n{context}\\n\\n\"\n        \"Answer:\"\n    )\n\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=900,\n        padding=True,\n    )\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            no_repeat_ngram_size=2,\n        )\n\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return text.split(\"Answer:\")[-1].strip()\n\ngenerated_answer = generate_answer_from_chunks(query, retrieved)\nprint(\"Query:\", query)\nprint(\"Generated answer:\\n\", generated_answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Basic evaluation (relevance, completeness, coherence)\n\nBelow uses the required example corpus/query/response format and computes:\n- **Relevance**: cosine similarity between query and response (TF-IDF)\n- **Completeness**: proportion of important query keywords covered in response\n- **Coherence**: perplexity of response under GPT-2 (lower is better)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "corpus = [\n    \"Distributed Data Parallel (DDP) allows multi-GPU training.\",\n    \"DDP synchronizes gradients across GPUs.\",\n    \"Using multiple GPUs increases training efficiency.\",\n    \"Parallelization across GPUs is useful for large models.\"\n]\n\nquery = \"What is Distributed Data Parallel (DDP) in PyTorch, and how is it useful in multi-GPU setups?\"\nresponse = (\n    \"Distributed Data Parallel (DDP) in PyTorch is a module that enables parallel \"\n    \"training across multiple GPUs by distributing model replicas and splitting data \"\n    \"across them.\"\n)\n\n# 1) Relevance via cosine similarity\nrel_vectorizer = TfidfVectorizer(stop_words=\"english\")\nqr_mat = rel_vectorizer.fit_transform([query, response])\nrelevance_score = cosine_similarity(qr_mat[0:1], qr_mat[1:2])[0, 0]\n\n# 2) Completeness via keyword coverage\ndef extract_keywords(text: str):\n    tokens = re.findall(r\"[a-zA-Z]+\", text.lower())\n    stop = {\n        \"what\", \"is\", \"in\", \"and\", \"how\", \"it\", \"the\", \"a\", \"an\", \"of\", \"to\", \"for\", \"on\", \"by\", \"with\"\n    }\n    return sorted(set([t for t in tokens if t not in stop and len(t) > 2]))\n\nquery_keywords = extract_keywords(query)\nresponse_tokens = set(re.findall(r\"[a-zA-Z]+\", response.lower()))\ncovered = [kw for kw in query_keywords if kw in response_tokens]\ncompleteness_score = len(covered) / max(1, len(query_keywords))\n\n# 3) Coherence via perplexity (lower is better)\ndef perplexity_gpt2(text: str):\n    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n    with torch.no_grad():\n        out = model(**enc, labels=enc[\"input_ids\"])\n    return float(torch.exp(out.loss))\n\ncoherence_perplexity = perplexity_gpt2(response)\n\nprint(f\"Relevance (cosine similarity): {relevance_score:.4f}\")\nprint(f\"Completeness (keyword coverage): {completeness_score:.4f}\")\nprint(f\"Coherence (GPT-2 perplexity, lower better): {coherence_perplexity:.4f}\")\nprint(\"\\nQuery keywords:\", query_keywords)\nprint(\"Covered keywords:\", covered)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Optional experiment: chunk size comparison\n\nTry different chunk sizes to observe retrieval-quality changes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_index_with_chunk_size(articles, chunk_size=120, overlap=20):\n    chunks = []\n    for a in articles:\n        chunks.extend(chunk_text_by_words(a.get(\"text\", \"\"), chunk_size=chunk_size, overlap=overlap))\n\n    vec = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n    mat = vec.fit_transform(chunks).toarray().astype(np.float32)\n    faiss.normalize_L2(mat)\n    idx = faiss.IndexFlatIP(mat.shape[1])\n    idx.add(mat)\n    return chunks, vec, idx\n\nfor size in [80, 120, 180]:\n    chunks_s, vec_s, idx_s = build_index_with_chunk_size(subset[:120], chunk_size=size, overlap=size//6)\n    q_vec = vec_s.transform([clean_text(\"what is france's capital city?\")]).toarray().astype(np.float32)\n    faiss.normalize_L2(q_vec)\n    scores, ids = idx_s.search(q_vec, 3)\n    print(f\"Chunk size={size}, top score={scores[0][0]:.4f}, chunks={len(chunks_s)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Qualitative observation template (for report/PPT)\n\n- **Relevant example:** Retrieved chunks explicitly mention terms from the query.\n- **Coherence strength:** GPT-2 output is usually fluent when context is clear.\n- **Common issue:** GPT-2 may add unsupported details if context is weak.\n- **Improvement ideas:** Better reranking, larger embedding model, prompt constraints, and answer post-validation."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}